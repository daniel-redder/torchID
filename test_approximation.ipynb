{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchID approximate search\n",
    "By: Daniel Redder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is the same stuff as in `test_simple.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchID import tag_model, identify_tensors, find_leaves\n",
    "from simple_model import SimpleModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load in some simple sleep data from kaggle: https://www.kaggle.com/code/tanshihjen/eda-timeseries-fitbitsleepscoredata/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall_score  revitalization_score  deep_sleep_in_minutes  \\\n",
      "0             83                    83                    104   \n",
      "1             87                    87                    114   \n",
      "2             84                    84                     99   \n",
      "3             81                    81                     73   \n",
      "4             76                    76                     64   \n",
      "\n",
      "   resting_heart_rate  restlessness  \n",
      "0                  63      0.068100  \n",
      "1                  63      0.053283  \n",
      "2                  64      0.051408  \n",
      "3                  65      0.046679  \n",
      "4                  65      0.076923  \n",
      "(291, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('sleep_score_data_fitbit.csv')[['overall_score', 'revitalization_score', 'deep_sleep_in_minutes', 'resting_heart_rate','restlessness']]\n",
    "print(df.head())\n",
    "\n",
    "print(df.values.shape)\n",
    "dataset = torch.tensor(df.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize our model, optimizer, and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "\n",
    "data_lab = dataset[:, -1].unsqueeze(1)\n",
    "data_lab = torch.nn.functional.normalize(data_lab, p=2, dim=1, eps=1e-12, out=None)\n",
    "model.train() #?this sets the requires_grads in some nn.modules to True \n",
    "\n",
    "#?but just to be really sure\n",
    "for n,m in model.named_parameters(): m.requires_grad = True\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to know which variables are leaves in this model, so to make this interesting lets slightly modify the output with additional leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.4803242683410645\n",
      "5.054694175720215\n",
      "4.308633804321289\n",
      "3.3790018558502197\n",
      "2.4075212478637695\n",
      "1.5183870792388916\n",
      "0.8027776479721069\n",
      "0.31126001477241516\n",
      "0.05371517688035965\n",
      "0.00536576472222805\n",
      "0.1168447807431221\n",
      "0.32603558897972107\n",
      "0.5695803761482239\n",
      "0.7924039363861084\n",
      "0.9542089700698853\n",
      "1.0325487852096558\n",
      "1.0226702690124512\n",
      "0.9347599148750305\n",
      "0.7894994616508484\n",
      "0.6129088997840881\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "#?note that this variable is not in the optimizer, or the model so we cannot locate it through the optimizer's parameter lists or the model's named_parameters\n",
    "delta = torch.FloatTensor([-1.4])\n",
    "delta.requires_grad = True\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #? get our model's predicted output\n",
    "    y_pred = model(dataset[:, :-1])\n",
    "\n",
    "    Y_pred = y_pred + delta\n",
    "\n",
    "    #? calculate the loss\n",
    "    loss = loss_fn(Y_pred, data_lab)\n",
    "    print(loss.item())\n",
    "\n",
    "    #?typical backpropogation commands\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the loss went down, but we still have this leaf `delta` with a lingering .grad value that is slowly accumulating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-9.4057])\n"
     ]
    }
   ],
   "source": [
    "print(delta.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This won't impact model's predictive performance, but it will impact memory usage, and especially with larger models `(specifically we had this happening in a diffusion meta-learning case)` it can make learning **intractable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, lets take the same loop and use **torchID** to find this tensor\n",
    "\n",
    "This is comprised of 3 functions:\n",
    "\n",
    "`find_leaves( grad_fn )`: this loops over `grad_fn.next_functions` to find `grad_fn` objects which have `.variable` attributes. It also records the path of `grad_fn's` taken to get to it **(used in our approximate search method)**\n",
    "\n",
    "`tag_model( torch.nn.Module )`: this is the straightforward solution it checks `model.named_parameters()`, and assigns `leaf.nmm = name` from `named_parameters` \n",
    "\n",
    "`identify_tensors( [tensor] )`: this approach finds tensors which are not parameters, but are leaves in the computational graph. This works by searching `sys.modules` i.e. it **searches all references defined in all loaded modules**. We explain why we choose to do it this way on the ReadME. To mitigate the overhead this includes we have a `limited_system_search` parameter which will look for whether a specific variable exists in each module before checking it for the tensor. \n",
    "\n",
    "i.e. \n",
    "\n",
    "```py \n",
    "-- other package --\n",
    "MyTestVar = ...\n",
    "\n",
    "-- other other package --\n",
    "(does not contain MyTestVar)\n",
    "\n",
    "-- main --\n",
    "identify_tensors(grad_fn, limited_system_search = \"MyTestVar\")\n",
    "```\n",
    "will only check in \"other package\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Overwritten Leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common case in torch packages is where a variable is overwritten by itself modified: `var = var+1`, now we cannot do `var+=1` in pytorch, but it does allow `var=var+1` which causes a problem for this package. \n",
    "\n",
    "Because tensors are immutable this change removes a reference to the original object `var`, and replaces it with a new `var* = var+1`. **This is a common spot for computational graph problems, so it is important we can do this.**\n",
    "\n",
    "To solve this we use a approximation approach where we find the nearest `grad_fn` \"before\" (in terms of backprop) the variable is overwritten, and we use that as the name. If the `identify_tensors` function encounters any of these cases it will print a warning.\n",
    "\n",
    "To do this, give the `outputDepthList` obj returned from `find_leaves` to `identify_tensors` as `approximate_search`. If a approximate is found you will see `approximate. ...` in the `nmm` attribute of the leaf with a measure of its seperation from the original leaf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5555741786956787\n",
      "\u001b[31mTensor object not found, using approximate search for the following objects: \u001b[0m\n",
      "\u001b[34mindex: 2, shape: torch.Size([1]), dtype: torch.float32\u001b[0m\n",
      "torch.Size([1]) None    ['SimpleModel_l1.bias', '__main__.m', '__main__.m', '__mp_main__.m', '__mp_main__.m']\n",
      "torch.Size([1, 4]) None    ['SimpleModel_l1.weight']\n",
      "torch.Size([1]) None    ['approximated.2seperated.__main__.delta']\n",
      "torch.Size([1]) None    ['SimpleModel_l1.bias', '__main__.m', '__main__.m', '__mp_main__.m', '__mp_main__.m']\n",
      "torch.Size([1, 4]) None    ['SimpleModel_l1.weight']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#?we only need one forward pass to build the computational graph\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #? get our model's predicted output\n",
    "    y_pred = model(dataset[:, :-1])\n",
    "\n",
    "    #!lets introduce some problems\n",
    "    delta = delta / y_pred\n",
    "\n",
    "    Y_pred = y_pred + delta\n",
    "\n",
    "    #? calculate the loss\n",
    "    loss = loss_fn(Y_pred, data_lab)\n",
    "    print(loss.item())\n",
    "\n",
    "    #?first we recursively find all the leaves of the computational graph this works by using the .variable bijective reference to the computational graph i.e. grad_fn.variable <-> tensor.grad_fn\n",
    "    #?leaves is a list of all the leaves of the computational graph (as tensors), paths is a list of lists of the paths to each leaf (grad_fn's)\n",
    "    leaves, paths = find_leaves(loss.grad_fn)\n",
    "\n",
    "\n",
    "    #?now we identify the tensors that are in the computational graph\n",
    "    #first we tag the model parameters so that they are handled seperately\n",
    "    tag_model(model)\n",
    "\n",
    "    #we then identify all leaves that are not model parameters\n",
    "    identify_tensors(leaves, approximate_search=paths)\n",
    "\n",
    "    for leaf in leaves: print(leaf.shape, leaf.grad_fn, \"  \",leaf.nmm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
