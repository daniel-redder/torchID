{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets see how torchID works with a simple regression model\n",
    "By: Daniel Redder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchID import tag_model, identify_tensors, find_leaves\n",
    "from simple_model import SimpleModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load in some simple sleep data from kaggle: https://www.kaggle.com/code/tanshihjen/eda-timeseries-fitbitsleepscoredata/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall_score  revitalization_score  deep_sleep_in_minutes  \\\n",
      "0             83                    83                    104   \n",
      "1             87                    87                    114   \n",
      "2             84                    84                     99   \n",
      "3             81                    81                     73   \n",
      "4             76                    76                     64   \n",
      "\n",
      "   resting_heart_rate  restlessness  \n",
      "0                  63      0.068100  \n",
      "1                  63      0.053283  \n",
      "2                  64      0.051408  \n",
      "3                  65      0.046679  \n",
      "4                  65      0.076923  \n",
      "(291, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('sleep_score_data_fitbit.csv')[['overall_score', 'revitalization_score', 'deep_sleep_in_minutes', 'resting_heart_rate','restlessness']]\n",
    "print(df.head())\n",
    "\n",
    "print(df.values.shape)\n",
    "dataset = torch.tensor(df.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize our model, optimizer, and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "\n",
    "data_lab = dataset[:, -1].unsqueeze(1)\n",
    "data_lab = torch.nn.functional.normalize(data_lab, p=2, dim=1, eps=1e-12, out=None)\n",
    "model.train() #?this sets the requires_grads in some nn.modules to True \n",
    "\n",
    "#?but just to be really sure\n",
    "for n,m in model.named_parameters(): m.requires_grad = True\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to know which variables are leaves in this model, so to make this interesting lets slightly modify the output with additional leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.235421180725098\n",
      "4.828830718994141\n",
      "4.116142749786377\n",
      "3.2280962467193604\n",
      "2.300071954727173\n",
      "1.450710654258728\n",
      "0.7671117186546326\n",
      "0.29758021235466003\n",
      "0.05155520886182785\n",
      "0.005367584060877562\n",
      "0.11185874789953232\n",
      "0.31169071793556213\n",
      "0.5443400740623474\n",
      "0.7571951746940613\n",
      "0.9117611050605774\n",
      "0.9865954518318176\n",
      "0.9771575331687927\n",
      "0.8931785225868225\n",
      "0.7544145584106445\n",
      "0.5857220888137817\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "#?note that this variable is not in the optimizer, or the model so we cannot locate it through the optimizer's parameter lists or the model's named_parameters\n",
    "delta = torch.FloatTensor([-1.4])\n",
    "delta.requires_grad = True\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #? get our model's predicted output\n",
    "    y_pred = model(dataset[:, :-1])\n",
    "\n",
    "    Y_pred = y_pred + delta\n",
    "\n",
    "    #? calculate the loss\n",
    "    loss = loss_fn(Y_pred, data_lab)\n",
    "    print(loss.item())\n",
    "\n",
    "    #?typical backpropogation commands\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the loss went down, but we still have this leaf `delta` with a lingering .grad value that is slowly accumulating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-9.1949])\n"
     ]
    }
   ],
   "source": [
    "print(delta.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This won't impact model's predictive performance, but it will impact memory usage, and especially with larger models `(specifically we had this happening in a diffusion meta-learning case)` it can make learning **intractable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, lets take the same loop and use **torchID** to find this tensor\n",
    "\n",
    "This is comprised of 3 functions:\n",
    "\n",
    "`find_leaves( grad_fn )`: this loops over `grad_fn.next_functions` to find `grad_fn` objects which have `.variable` attributes. It also records the path of `grad_fn's` taken to get to it **(used in our approximate search method)**\n",
    "\n",
    "`tag_model( torch.nn.Module )`: this is the straightforward solution it checks `model.named_parameters()`, and assigns `leaf.nmm = name` from `named_parameters` \n",
    "\n",
    "`identify_tensors( [tensor] )`: this approach finds tensors which are not parameters, but are leaves in the computational graph. This works by searching `sys.modules` i.e. it **searches all references defined in all loaded modules**. We explain why we choose to do it this way on the ReadME. To mitigate the overhead this includes we have a `limited_system_search` parameter which will look for whether a specific variable exists in each module before checking it for the tensor. \n",
    "\n",
    "i.e. \n",
    "\n",
    "```py \n",
    "-- other package --\n",
    "MyTestVar = ...\n",
    "\n",
    "-- other other package --\n",
    "(does not contain MyTestVar)\n",
    "\n",
    "-- main --\n",
    "identify_tensors(grad_fn, limited_system_search = \"MyTestVar\")\n",
    "```\n",
    "will only check in \"other package\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41231611371040344\n",
      "torch.Size([1]) None    ['SimpleModel_l1.bias', '__main__.m', '__mp_main__.m']\n",
      "torch.Size([1, 4]) None    ['SimpleModel_l1.weight']\n",
      "torch.Size([1]) None    ['__main__.delta', '__mp_main__.delta']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#?we only need one forward pass to build the computational graph\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    #? get our model's predicted output\n",
    "    y_pred = model(dataset[:, :-1])\n",
    "\n",
    "    Y_pred = y_pred + delta\n",
    "\n",
    "    #? calculate the loss\n",
    "    loss = loss_fn(Y_pred, data_lab)\n",
    "    print(loss.item())\n",
    "\n",
    "    #?first we recursively find all the leaves of the computational graph this works by using the .variable bijective reference to the computational graph i.e. grad_fn.variable <-> tensor.grad_fn\n",
    "    #?leaves is a list of all the leaves of the computational graph (as tensors), paths is a list of lists of the paths to each leaf (grad_fn's)\n",
    "    leaves, paths = find_leaves(loss.grad_fn)\n",
    "\n",
    "\n",
    "    #?now we identify the tensors that are in the computational graph\n",
    "    #first we tag the model parameters so that they are handled seperately\n",
    "    tag_model(model)\n",
    "\n",
    "    #we then identify all leaves that are not model parameters\n",
    "    identify_tensors(leaves)\n",
    "\n",
    "    for leaf in leaves: print(leaf.shape, leaf.grad_fn, \"  \",leaf.nmm)\n",
    "\n",
    "    #?typical backpropogation commands\n",
    "    # optim.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the output has a list of variable names, this is because we locate all references to the leaf not just the time when it is used. This is because torch tensors are immutable objects and this is a constraint of our method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
